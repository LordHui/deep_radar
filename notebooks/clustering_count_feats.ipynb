{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sk7898/deep_radar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from data import get_fft_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Metrics\n",
    "\n",
    "Reference: https://gemfury.com/stream/python:scikit-learn/-/content/metrics/cluster/supervised.py\n",
    "\n",
    "**homogeneity_score**:\n",
    "A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "- Perfect labelings are homogeneous\n",
    "```python\n",
    "homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0]) --> 1.0\n",
    "```\n",
    "- Non-perfect labelings that further split classes into more clusters can be perfectly homogeneous\n",
    "```python\n",
    "homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]) --> 1.000000\n",
    "```\n",
    "- Clusters that include samples from different classes do not make for an homogeneous labeling\n",
    "```python\n",
    "homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]) --> 0.0...\n",
    "```\n",
    "***\n",
    "\n",
    "**completeness_score**:\n",
    "A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "- Perfect labelings are complete\n",
    "- Non-perfect labelings that assign all classes members to the same clusters are still complete\n",
    "```python \n",
    "completeness_score([0, 0, 1, 1], [0, 0, 0, 0]) --> 1.0 \n",
    "completeness_score([0, 1, 2, 3], [0, 0, 1, 1]) --> 0.999...\n",
    "```\n",
    "- If classes members are split across different clusters, the assignment cannot be complete\n",
    "```python\n",
    "completeness_score([0, 0, 1, 1], [0, 1, 0, 1]) --> 0.0\n",
    "```\n",
    "***\n",
    "\n",
    "**mutual_info_score**:\n",
    "Mutual Information between two clusterings. The Mutual Information is a measure of the similarity between two labels of the same data. Where ${|U_i|}$ is the number of the samples in cluster $U_i$ and ${|V_j|}$ is the number of the samples in cluster $V_j$, the Mutual Information between clusterings U and V is given as:\n",
    "\n",
    "<center> \n",
    "    $MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}$ \n",
    "</center>\n",
    "\n",
    "This metric is furthermore symmetric: switching ``label_true`` with ``label_pred`` will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.\n",
    "    \n",
    "***\n",
    "\n",
    "**contingency_matrix:**\n",
    "Matrix C such that $C_{i,j}$ is the number of samples in true class i and in predicted class j. \n",
    "\n",
    "***\n",
    "\n",
    "**v_measure_score**:\n",
    "harmonic mean between homogeneity and completeness:\n",
    "<center>\n",
    "$v = (1 + beta) * homogeneity * completeness / (beta * homogeneity + completeness)$\n",
    "</center>\n",
    "\n",
    "- Perfect labelings are both homogeneous and complete, hence have score 1.0\n",
    "```python\n",
    "v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]) --> 1.0\n",
    "```\n",
    "\n",
    "- Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized\n",
    "```python\n",
    "v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]) --> 0.8...\n",
    "v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]) --> 0.66...\n",
    "```\n",
    "\n",
    "- Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well\n",
    "```python\n",
    "v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]) --> 0.66...\n",
    "```\n",
    "\n",
    "- If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null\n",
    "```python\n",
    "v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]) --> 0.0...\n",
    "```\n",
    "\n",
    "- Clusters that include samples from totally different classes totally destroy the homogeneity of the labeling\n",
    "```python\n",
    "v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]) --> 0.0...\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "**adjusted_rand_score**:\n",
    "The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:\n",
    "<center>\n",
    "$ARI = (RI - Expected\\_RI) / (max(RI) - Expected\\_RI)$\n",
    "</center>\n",
    "\n",
    "- Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized\n",
    "```python\n",
    "adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1]) --> 0.57...\n",
    "```\n",
    "- ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized\n",
    "```python\n",
    "adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2]) --> 0.57...\n",
    "```\n",
    "- If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low\n",
    "```python\n",
    "adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3]) --> 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_homogeneous_clusters(y_true, cluster_labels, n_clusters):\n",
    "    homogeneous_clusters = []\n",
    "    scores = np.zeros(n_clusters, dtype=np.float)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        indexes = cluster_labels == i\n",
    "        scores[i] = metrics.cluster.homogeneity_score(y_true[indexes], cluster_labels[indexes])\n",
    "        if scores[i] > 0.8:\n",
    "            homogeneous_clusters.append(i)\n",
    "\n",
    "    return homogeneous_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(y, cluster_labels, n_clusters=4):\n",
    "    if y.ndim != 1:\n",
    "        y = y.flatten()\n",
    "        \n",
    "    h_clusters = get_n_homogeneous_clusters(y,\n",
    "                                            cluster_labels, \n",
    "                                            n_clusters=n_clusters)\n",
    "    \n",
    "    print(\"Homogeneity:\", metrics.homogeneity_score(y, cluster_labels))\n",
    "    print('Homogeneous Clusters: ', len(np.unique(h_clusters)))\n",
    "    print(\"Mutual Information: \", metrics.mutual_info_score(y, cluster_labels))\n",
    "    \n",
    "    if n_clusters == 4:\n",
    "        print(\"Completeness:\", metrics.completeness_score(y, cluster_labels))\n",
    "        print(\"V-measure:\", metrics.v_measure_score(y, cluster_labels))\n",
    "        print(\"Adjusted Rand-Index: \", metrics.cluster.adjusted_rand_score(y, cluster_labels))   \n",
    "        print(\"Contingency Matrix: \\n\", metrics.cluster.contingency_matrix(y, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid_cluster(X):\n",
    "    length = X.shape[0]\n",
    "    sum_x = np.sum(X[:, 0])\n",
    "    sum_y = np.sum(X[:, 1])\n",
    "    return sum_x/length, sum_y/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(X, y, n_classes=2):\n",
    "    cents = np.empty((n_classes, 2))\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        X_subset = X[y == i]\n",
    "        cents[i, :] = get_centroid_cluster(X_subset)\n",
    "    \n",
    "    return cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inc_seqs(seqs, inc):\n",
    "    inc_seqs = [seqs[idx].split('_') for idx in inc]\n",
    "    fraction = [x[0] +'_'+ str(np.round(int(x[-1])/int(x[1]), 3)) for x in inc_seqs]\n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_mode(y, clust_labels, n_clusters=2):\n",
    "    lbls_lst = [[] for i in range(n_clusters)]\n",
    "    modes = np.empty((n_clusters), dtype=np.int8)\n",
    "\n",
    "    for t, p in zip(y, clust_labels):\n",
    "        lbls_lst[int(p)].append(t)\n",
    "\n",
    "    for idx, lst in enumerate(lbls_lst):\n",
    "        modes[idx] = stats.mode(lst, axis=None)[0][0]\n",
    "        \n",
    "    return modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_incorrect_idx(y, cls_pred, n_classes=2, print_res=False):\n",
    "    c_idx = []\n",
    "    inc_idx = []\n",
    "    c_lst = [[] for i in range(n_classes)]\n",
    "    inc_lst = [[] for i in range(n_classes)]\n",
    "\n",
    "    for idx, (t, pred) in enumerate(zip(y, cls_pred)):\n",
    "        if t[0] != pred:\n",
    "            inc_idx.append(idx)\n",
    "            inc_lst[int(t)].append(idx)\n",
    "        else:\n",
    "            c_idx.append(idx)\n",
    "            c_lst[int(t)].append(idx)\n",
    "\n",
    "    if print_res:\n",
    "        if n_classes == 2:\n",
    "            print('Correct:{}, 1:{}, 2:{}'.format(len(y) - len(inc_idx),\n",
    "                                                  len(c_lst[0]),\n",
    "                                                  len(c_lst[1])))\n",
    "            \n",
    "            print('Incorrect:{}, 1:{}, 2:{}'.format(len(inc_idx),\n",
    "                                                    len(inc_lst[0]),\n",
    "                                                    len(inc_lst[1])))\n",
    "        elif n_classes == 3:\n",
    "            print('Correct:{}, 1:{}, 2:{}, 3:{}'.format(len(y) - len(inc_idx),\n",
    "                                                        len(c_lst[0]),\n",
    "                                                        len(c_lst[1]),\n",
    "                                                        len(c_lst[2])))\n",
    "            \n",
    "            print('Incorrect:{}, 1:{}, 2:{}, 3:{}'.format(len(inc_idx),\n",
    "                                                          len(inc_lst[0]),\n",
    "                                                          len(inc_lst[1]),\n",
    "                                                          len(inc_lst[2])))           \n",
    "        elif n_classes == 4:\n",
    "            print('Correct:{}, 1:{}, 2:{}, 3:{}, 4:{}'.format(len(y) - len(inc_idx),\n",
    "                                                              len(c_lst[0]),\n",
    "                                                              len(c_lst[1]),\n",
    "                                                              len(c_lst[2]),\n",
    "                                                              len(c_lst[3])))\n",
    "            \n",
    "            print('Incorrect:{}, 1:{}, 2:{}, 3:{} 4:{}'.format(len(inc_idx),\n",
    "                                                               len(inc_lst[0]),\n",
    "                                                               len(inc_lst[1]),\n",
    "                                                               len(inc_lst[2]),\n",
    "                                                               len(inc_lst[3])))\n",
    "        else:\n",
    "            print('Number of Classes not supported till now')\n",
    "        \n",
    "    return c_idx, inc_idx, c_lst, inc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_data(X, y, old_y, idxs):\n",
    "    X_c = [X[idx] for idx in idxs]\n",
    "    y_c = [y[idx] for idx in idxs]\n",
    "    old_y_c = [old_y[idx] for idx in idxs]\n",
    "    \n",
    "    return np.array(X_c), np.array(y_c).flatten(), np.array(old_y_c).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(axs, X, y, \n",
    "              palette, \n",
    "              row_idx, col_idx, \n",
    "              y_annotate=None, \n",
    "              cents=None, \n",
    "              legend='full'):\n",
    "    \n",
    "    p1 = sns.scatterplot(X[:,0], X[:,1],\n",
    "                         hue=y,\n",
    "                         legend=legend,\n",
    "                         palette=palette,\n",
    "                         ax=axs[row_idx, col_idx])\n",
    "\n",
    "    if y_annotate is not None:\n",
    "        for i, c in enumerate(cents):\n",
    "            p1.text(c[0], c[1], \n",
    "                    y_annotate[i], \n",
    "                    horizontalalignment='left',\n",
    "                    size='medium',\n",
    "                    color='black',\n",
    "                    weight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_fig(rows, cols=None, col_names=None):\n",
    "    sns.set(rc={'figure.figsize':(16, 20)})    \n",
    "    fig, axs = plt.subplots(nrows=rows, ncols=cols)\n",
    "\n",
    "    if col_names:\n",
    "        for ax, col in zip(axs[0], col_names):\n",
    "            ax.set_title(col)   \n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relabel_dict(X, X_test, y_test, n_clusters):\n",
    "    relabel_dict = {1: [0, 0, 0, 0], 2: [0, 0, 0, 0], 3: [0, 0, 0, 0], 4: [0, 0, 0, 0]}\n",
    "\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    clust_labels = kmeans.predict(X_test)\n",
    "    clust_true_labels = get_cluster_mode(y_test, clust_labels, n_clusters=n_clusters)\n",
    "\n",
    "    new_labels = [clust_true_labels[cluster_id] for cluster_id in clust_labels]\n",
    "\n",
    "    for old_y, new_y in zip(y_test, new_labels):\n",
    "        relabel_dict[old_y][new_y-1] += 1\n",
    "    \n",
    "    for key in relabel_dict.keys():\n",
    "        print('True Label:{} Relabel:{}'.format(key, relabel_dict[key]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN**\n",
    "\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) locates regions of high density that\n",
    "are separated from one another by regions of low density. If there are at least min_samples many data points \n",
    "within a distance of eps to a given data point, that data point is classified as a core sample\n",
    "core samples that are closer to each other than the distance eps are put into the same cluster by DBSCAN.\\\n",
    "*Density at a point P*: Number of points within a circle of Radius *eps (ϵ)* from point P.\\\n",
    "*Dense Region*: For each point in the cluster, the circle with radius ϵ contains at least minimum number of points *min_samples*.\n",
    "\n",
    "<div>\n",
    "<img src=\"dbscan.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_dbscan(X_train, y_train,\n",
    "                      min_samples_lst=None,\n",
    "                      eps_lst=None,\n",
    "                      pca_train=True):\n",
    "\n",
    "    pca = PCA(n_components=2).fit(X_train)\n",
    "    X = pca.transform(X_train)\n",
    "    X_train = X if pca_train else X_train\n",
    "\n",
    "    if eps_lst and min_samples_lst:\n",
    "        col_names = ['min_samples:'+ str(min_samples)+' eps:' + str(eps) for min_samples in min_samples_lst\n",
    "                                                                         for eps in eps_list \n",
    "                    ]\n",
    "        \n",
    "        fig, axs = get_plot_fig(rows=len(min_samples_lst),\n",
    "                                cols=len(eps_lst),\n",
    "                                col_names=col_names)\n",
    "        \n",
    "        for row_idx, min_samples in enumerate(min_samples_lst):\n",
    "            for col_idx, eps in enumerate(eps_lst):\n",
    "                db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_train)\n",
    "                core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "                core_samples_mask[db.core_sample_indices_] = True\n",
    "                labels = db.labels_\n",
    "                \n",
    "                n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                n_noise_ = list(labels).count(-1)\n",
    "\n",
    "                print('Estimated number of clusters: %d' % n_clusters_)\n",
    "                print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "                labels_palette = sns.color_palette(\"bright\", len(np.unique(labels)))\n",
    "        \n",
    "                plot_data(axs, X, labels,\n",
    "                          palette=labels_palette,\n",
    "                          row_idx=row_idx,\n",
    "                          col_idx=col_idx,\n",
    "                          legend='full')\n",
    "            \n",
    "                print_scores(y_train, labels)\n",
    "                \n",
    "    else:\n",
    "        db = DBSCAN(eps=0.3, min_samples=10).fit(X_train)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "\n",
    "        # DBSCAN helps us to identify noise in the data.\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "        print('Estimated number of clusters: %d' % n_clusters_)\n",
    "        print('Estimated number of noise points: %d' % n_noise_)\n",
    "        \n",
    "        print_scores(y_train, labels)\n",
    "        labels_palette = sns.color_palette(\"bright\", len(np.unique(labels)))\n",
    "        \n",
    "        sns.scatterplot(X[:, 0], X[:, 1], hue=labels, palette=labels_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_agglomerative(X_train, y_train, \n",
    "                             affinity,\n",
    "                             linkage,\n",
    "                             n_clusters,\n",
    "                             pca_train=True):\n",
    "    \n",
    "    # Create a graph capturing local connectivity. Larger number of neighbors\n",
    "    # will give more homogeneous clusters to the cost of computation\n",
    "    # time. A very large number of neighbors gives more evenly distributed\n",
    "    # cluster sizes, but may not impose the local manifold structure of\n",
    "    # the data\n",
    "    \n",
    "    pca = PCA(n_components=2).fit(X_train)\n",
    "    X = pca.transform(X_train)\n",
    "    \n",
    "    X_train = X if pca_train else X_train\n",
    "    \n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(X_train, n_neighbors=10, include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    agg = AgglomerativeClustering(linkage=linkage,\n",
    "                                  affinity=affinity,\n",
    "                                  connectivity=connectivity,\n",
    "                                  n_clusters=n_clusters)\n",
    "    agg.fit(X_train)\n",
    "     \n",
    "    # Print the clustering scores\n",
    "    print_scores(y_train, agg.labels_)\n",
    "    \n",
    "    labels_palette = sns.color_palette(\"bright\", len(np.unique(agg.labels_)))\n",
    "    sns.scatterplot(X[:, 0], X[:, 1], hue=agg.labels_, palette=labels_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_kmeans(X_train, y, old_y, \n",
    "                      X_test, y_test, old_y_test,\n",
    "                      cluster_list,\n",
    "                      col_names,\n",
    "                      sel_cls,\n",
    "                      pca_train=True):\n",
    "    \n",
    "    def kmeans_train(X, y, old_y,\n",
    "                     n_clusters, \n",
    "                     n_classes,\n",
    "                     cluster_palette,\n",
    "                     row_idx, col_idx,\n",
    "                     plot=True):\n",
    "\n",
    "        kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=n_classes)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        clust_labels = kmeans.predict(X)        \n",
    "        cent = kmeans.cluster_centers_\n",
    "        clust_true_labels = get_cluster_mode(old_y, clust_labels, n_clusters=n_clusters)\n",
    "\n",
    "        # print the clustering scores \n",
    "        print_scores(y, clust_labels, n_clusters=n_clusters)\n",
    "        \n",
    "        if plot:\n",
    "            plot_data(axs, X=X, y=clust_labels,\n",
    "                      palette=cluster_palette,\n",
    "                      y_annotate=clust_true_labels,\n",
    "                      cents=cent,\n",
    "                      legend=False,\n",
    "                      row_idx=row_idx, col_idx=col_idx)\n",
    "        \n",
    "            return kmeans, col_idx+1\n",
    "        \n",
    "        else:\n",
    "            return kmeans, col_idx\n",
    "            \n",
    "    def kmeans_predict(kmeans, X, old_y,\n",
    "                       n_clusters,\n",
    "                       cluster_palette,\n",
    "                       row_idx, col_idx,\n",
    "                       plot=True):\n",
    "        \n",
    "        clust_labels = kmeans.predict(X)\n",
    "        cent_X = kmeans.cluster_centers_\n",
    "        clust_true_labels = get_cluster_mode(old_y, clust_labels, n_clusters=n_clusters)\n",
    "\n",
    "        if plot:\n",
    "            plot_data(axs, X=X, y=clust_labels,\n",
    "                      palette=cluster_palette,\n",
    "                      y_annotate=clust_true_labels,\n",
    "                      cents=cent_X,\n",
    "                      legend=False,\n",
    "                      row_idx=row_idx, col_idx=col_idx)\n",
    "        \n",
    "            return clust_labels, col_idx+1\n",
    "        \n",
    "        else:\n",
    "            return clust_labels, col_idx\n",
    "        \n",
    "    n_classes = len(sel_cls) \n",
    "    plot = True if pca_train else False\n",
    "    \n",
    "    if plot:\n",
    "        fig, axs = get_plot_fig(rows=len(cluster_list),\n",
    "                                cols=len(col_names),\n",
    "                                col_names=col_names)\n",
    "        true_palette = sns.color_palette(\"bright\", n_classes)\n",
    "\n",
    "    pca = PCA(n_components=2).fit(X_train)\n",
    "    reduced_train = pca.transform(X_train)\n",
    "    reduced_test = pca.transform(X_test)\n",
    "    \n",
    "    X_train_c, y_train_c, old_y_train_c = get_subset_data(X=X_train,\n",
    "                                                          y=y,\n",
    "                                                          old_y=old_y,\n",
    "                                                          idxs=c_idx)\n",
    "        \n",
    "    X_c, y_c, old_y_c = get_subset_data(X=reduced_train,\n",
    "                                        y=y,\n",
    "                                        old_y=old_y,\n",
    "                                        idxs=c_idx)\n",
    "\n",
    "    X_inc, y_inc, old_y_inc = get_subset_data(X=reduced_train,\n",
    "                                              y=y,\n",
    "                                              old_y=old_y,\n",
    "                                              idxs=inc_idx)\n",
    "    \n",
    "    for i, n_clusters in enumerate(cluster_list):\n",
    "        col_idx = 0\n",
    "        cluster_palette = sns.color_palette(\"bright\", n_clusters)\n",
    "\n",
    "        print('***********************n_clusters={}*******************************'.format(n_clusters))\n",
    "        \n",
    "        if plot and 'X_train(PCA)' in col_names:\n",
    "            # Plot the PCA reduced X_train\n",
    "            cent_train = get_centroids(reduced_train, old_y, n_classes=n_classes)\n",
    "            plot_data(axs, X=reduced_train, y=old_y,\n",
    "                      palette=true_palette,\n",
    "                      y_annotate=sel_cls,\n",
    "                      cents=cent_train,\n",
    "                      row_idx=i, col_idx=col_idx)\n",
    "            col_idx += 1\n",
    "            \n",
    "        if 'kmeans trained with X_train' in col_names:\n",
    "            X = reduced_train if pca_train else X_train\n",
    "            \n",
    "            print('Clustering Scores on X_train when trained with X_train')\n",
    "            kmeans, col_idx = kmeans_train(X, y_train, old_y,\n",
    "                                           n_clusters=n_clusters,\n",
    "                                           n_classes=n_classes,\n",
    "                                           cluster_palette=cluster_palette,\n",
    "                                           row_idx=i, col_idx=col_idx,\n",
    "                                           plot=plot)\n",
    "            print('------------------------------------------------------------------------')\n",
    "            \n",
    "        if 'kmeans trained with X_c' in col_names:\n",
    "            X = X_c if pca_train else X_train_c\n",
    "            X_train = reduced_train if pca_train else X_train\n",
    "            \n",
    "            print('Clustering Scores on X_c when trained with X_c')\n",
    "            kmeans, col_idx = kmeans_train(X, y_c, old_y_c,\n",
    "                                           n_clusters=n_clusters,\n",
    "                                           n_classes=n_classes,\n",
    "                                           cluster_palette=cluster_palette,\n",
    "                                           row_idx=i, col_idx=col_idx,\n",
    "                                           plot=plot)\n",
    "            print('------------------------------------------------------------------------')\n",
    "            \n",
    "            print('Clustering Scores on X_train when trained with X_c')\n",
    "            # print the clustering scores\n",
    "            print_scores(y_train, kmeans.predict(X_train), n_clusters=n_clusters)\n",
    "            print('------------------------------------------------------------------------')\n",
    "            \n",
    "        if plot and 'X_inc(PCA)' in col_names:\n",
    "            # Plot the X_train samples which were misclassified by LSTM\n",
    "            plot_data(axs, X=X_inc, y=old_y_inc,\n",
    "                      palette=true_palette,\n",
    "                      row_idx=i, col_idx=col_idx)\n",
    "            col_idx += 1\n",
    "\n",
    "        if 'kmeans predict X_inc' in col_names:\n",
    "            # Plot cluster predictions on X_inc\n",
    "            clust_labels, col_idx = kmeans_predict(kmeans, X_inc, old_y_inc,\n",
    "                                                   n_clusters=n_clusters,\n",
    "                                                   cluster_palette=cluster_palette,\n",
    "                                                   row_idx=i, col_idx=col_idx,\n",
    "                                                   plot=plot)\n",
    "            \n",
    "        if plot and 'X_test(PCA)' in col_names:\n",
    "            # Plot the PCA reduced X_test\n",
    "            cent_test = get_centroids(reduced_test, y_test, n_classes=n_classes)\n",
    "            plot_data(axs, X=reduced_test, y=old_y_test,\n",
    "                      palette=true_palette,\n",
    "                      y_annotate=sel_cls,\n",
    "                      cents=cent_test,\n",
    "                      row_idx=i, col_idx=col_idx)\n",
    "            col_idx += 1\n",
    "\n",
    "        if 'kmeans predict X_test' in col_names:\n",
    "            X = reduced_test if pca_train else X_test\n",
    "            \n",
    "            clust_labels_test, col_idx = kmeans_predict(kmeans, X, old_y_test,\n",
    "                                                        n_clusters=n_clusters,\n",
    "                                                        cluster_palette=cluster_palette,\n",
    "                                                        row_idx=i, col_idx=col_idx,\n",
    "                                                        plot=plot)\n",
    "                            \n",
    "            # print the clustering scores \n",
    "            print('Clustering Scores on X_test')\n",
    "            print_scores(y_test, clust_labels_test, n_clusters=n_clusters)\n",
    "            print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************n_clusters=20*******************************\n",
      "Clustering Scores on X_c when trained with X_c\n",
      "Homogeneity: 0.9493304975436917\n",
      "Homogeneous Clusters:  7\n",
      "Mutual Information:  1.2967430690207222\n",
      "------------------------------------------------------------------------\n",
      "Clustering Scores on X_train when trained with X_c\n",
      "Homogeneity: 0.8739611243814985\n",
      "Homogeneous Clusters:  1\n",
      "Mutual Information:  1.1927021627826266\n",
      "------------------------------------------------------------------------\n",
      "***********************n_clusters=50*******************************\n",
      "Clustering Scores on X_c when trained with X_c\n",
      "Homogeneity: 0.9813594610560762\n",
      "Homogeneous Clusters:  36\n",
      "Mutual Information:  1.3404932029836214\n",
      "------------------------------------------------------------------------\n",
      "Clustering Scores on X_train when trained with X_c\n",
      "Homogeneity: 0.9030247232992605\n",
      "Homogeneous Clusters:  7\n",
      "Mutual Information:  1.2323655028562408\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "method = 'kmeans'\n",
    "model_dir = '/scratch/sk7898/pedbike/models/lstm/'\n",
    "cls_str_list = ['1_2_3_4']\n",
    "sel_cls_list = [[1, 2, 3, 4]]\n",
    "layer_name = 'counting_dense_2'\n",
    "pca_train = False\n",
    "\n",
    "for idx, (cls_str, sel_cls) in enumerate(zip(cls_str_list, sel_cls_list)):\n",
    "    model_str = os.path.join(cls_str + '_amp_512_hidden_128/best_model.h5')\n",
    "    model_path = os.path.join(model_dir, model_str)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, old_y, old_y_test, _, _ = get_fft_data(sel_cls=sel_cls,\n",
    "                                                                             data_mode='amp')\n",
    "    \n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    old_y = old_y.flatten()\n",
    "    old_y_test = old_y_test.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "    model = load_model(model_path)\n",
    "    pred = model.predict(x=X)\n",
    "    cls_pred = np.argmax(pred, axis = 1)\n",
    "    c_idx, inc_idx, c_lst, inc_lst = get_correct_incorrect_idx(y, \n",
    "                                                               cls_pred, \n",
    "                                                               n_classes=len(sel_cls)) \n",
    "    \n",
    "    model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "    emb_train = model.predict(x=X)\n",
    "    emb_test = model.predict(x=X_test)\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        cluster_list = [20, 50]\n",
    "        col_names = [#'X_train(PCA)',\n",
    "                     #'kmeans trained with X_train',\n",
    "                     'kmeans trained with X_c',\n",
    "                     #'kmeans predict X_test'\n",
    "                    ]\n",
    "\n",
    "        clustering_kmeans(emb_train, y, old_y,\n",
    "                          emb_test, y_test, old_y_test,\n",
    "                          cluster_list=cluster_list,\n",
    "                          col_names=col_names,\n",
    "                          sel_cls=sel_cls,\n",
    "                          pca_train=pca_train\n",
    "                         )\n",
    "        \n",
    "    if method == 'DBSCAN':\n",
    "        min_samples_lst = [2, 5, 10]\n",
    "        eps_lst = [0.3, 0.5, 1.0, 1.5]\n",
    "        clustering_dbscan(emb_train, y,\n",
    "                          min_samples_lst=min_samples_lst,\n",
    "                          eps_lst=eps_lst,\n",
    "                          pca_train=pca_train\n",
    "                         )\n",
    "        \n",
    "    if method == 'Agglomerative':\n",
    "        clustering_agglomerative(emb_train, y,\n",
    "                                 affinity='euclidean',\n",
    "                                 linkage='ward',  \n",
    "                                 n_clusters=8,\n",
    "                                 pca_train=pca_train\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
