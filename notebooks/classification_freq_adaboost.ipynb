{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbls_for_cls(labels, lbls_list=None):\n",
    "    new_labels = [i for i in range(len(lbls_list))]\n",
    "    for i, lbl in enumerate(lbls_list):\n",
    "        labels[labels == lbl] = new_labels[i]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sel_cls,\n",
    "             data_mode='amp', \n",
    "             task_type='cls',\n",
    "             scaling=True):\n",
    "    \n",
    "    data_dir = '/scratch/sk7898/pedbike/fft_data'\n",
    "    data_path = os.path.join(data_dir, 'Data_win_fft.npy')\n",
    "    labels_path = os.path.join(data_dir, 'label_win_fft.npy')\n",
    "    seqs_path = os.path.join(data_dir, 'seqs_fft.npy')\n",
    "    data = np.load(data_path, allow_pickle=True) #shape: (18642, 256, 5)\n",
    "    labels = np.load(labels_path, allow_pickle=True) #shape: (18642,)\n",
    "    seqs = np.load(seqs_path, allow_pickle=True) #shape: (18642,)\n",
    "\n",
    "    n_data = data.swapaxes(1, 2)\n",
    "    amp_data = np.absolute(n_data)\n",
    "    phase_data = np.angle(n_data)\n",
    "    power_data = np.absolute(n_data)**2\n",
    "    real_data = np.real(n_data)\n",
    "    imag_data = np.imag(n_data)\n",
    "    \n",
    "    if data_mode == 'amp':\n",
    "        data = amp_data\n",
    "    elif data_mode == 'phase':\n",
    "        data = phase_data\n",
    "    elif data_mode == 'power':\n",
    "        data == power_data\n",
    "\n",
    "    labels_idx = []\n",
    "\n",
    "    if len(sel_cls) < 6:\n",
    "        for cls in sel_cls:\n",
    "            labels_idx += np.argwhere(labels == cls).flatten().tolist()\n",
    "            \n",
    "        sel_labels = [labels[idx] for idx in labels_idx]\n",
    "        sel_data = [data[idx] for idx in labels_idx]\n",
    "        sel_seqs = [seqs[idx] for idx in labels_idx]\n",
    "        labels = np.array(sel_labels)\n",
    "        data = np.array(sel_data)\n",
    "        seqs = np.array(sel_seqs)\n",
    "\n",
    "    if task_type == 'reg':\n",
    "        labels = labels.reshape(-1, 1) \n",
    "    else:\n",
    "        labels = lbls_for_cls(labels, lbls_list=sel_cls)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "    data = data.astype(np.float32)    \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, seqs_train, seqs_test = train_test_split(data, \n",
    "                                                                               labels,\n",
    "                                                                               seqs,\n",
    "                                                                               test_size=0.1,\n",
    "                                                                               random_state=42)\n",
    "    \n",
    "    if scaling:\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train.reshape(X_train.shape[0], -1))\n",
    "        X_train = scaler.transform(X_train.reshape(X_train.shape[0], -1))\n",
    "        X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "        X_train = X_train.reshape(X_train.shape[0], n_data.shape[1], n_data.shape[2])\n",
    "        X_test = X_test.reshape(X_test.shape[0], n_data.shape[1], n_data.shape[2])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_fft_model(hidden_1,\n",
    "                         counting_dense_1,\n",
    "                         counting_dense_2,\n",
    "                         kernel_initializer='normal',\n",
    "                         dropout=None,\n",
    "                         optimizer=None,\n",
    "                         input_shape=(5, 256),\n",
    "                         n_classes=2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_1, return_sequences=False, input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(counting_dense_1, activation='relu', name='counting_dense_1'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(counting_dense_2, activation='relu', name='counting_dense_2'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_classes, activation='softmax', name='output'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['sparse_categorical_accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10473 samples, validate on 1164 samples\n",
      "Epoch 1/20\n",
      "10473/10473 [==============================] - 4s 423us/step - loss: 9.9404e-05 - sparse_categorical_accuracy: 0.4051 - val_loss: 8.8570e-05 - val_sparse_categorical_accuracy: 0.4880\n",
      "Epoch 2/20\n",
      "10473/10473 [==============================] - 4s 363us/step - loss: 8.9522e-05 - sparse_categorical_accuracy: 0.4712 - val_loss: 8.1779e-05 - val_sparse_categorical_accuracy: 0.5533\n",
      "Epoch 3/20\n",
      "10473/10473 [==============================] - 3s 313us/step - loss: 8.3639e-05 - sparse_categorical_accuracy: 0.5212 - val_loss: 7.7621e-05 - val_sparse_categorical_accuracy: 0.5679\n",
      "Epoch 4/20\n",
      "10473/10473 [==============================] - 4s 375us/step - loss: 7.9935e-05 - sparse_categorical_accuracy: 0.5472 - val_loss: 7.5252e-05 - val_sparse_categorical_accuracy: 0.5816\n",
      "Epoch 5/20\n",
      "10473/10473 [==============================] - 4s 376us/step - loss: 7.7014e-05 - sparse_categorical_accuracy: 0.5714 - val_loss: 7.3608e-05 - val_sparse_categorical_accuracy: 0.6074\n",
      "Epoch 6/20\n",
      "10473/10473 [==============================] - 4s 377us/step - loss: 7.4752e-05 - sparse_categorical_accuracy: 0.5824 - val_loss: 7.1806e-05 - val_sparse_categorical_accuracy: 0.6151\n",
      "Epoch 7/20\n",
      "10473/10473 [==============================] - 4s 380us/step - loss: 7.3150e-05 - sparse_categorical_accuracy: 0.5998 - val_loss: 7.0397e-05 - val_sparse_categorical_accuracy: 0.6314\n",
      "Epoch 8/20\n",
      "10473/10473 [==============================] - 4s 388us/step - loss: 7.1020e-05 - sparse_categorical_accuracy: 0.6202 - val_loss: 6.8673e-05 - val_sparse_categorical_accuracy: 0.6503\n",
      "Epoch 9/20\n",
      "10473/10473 [==============================] - 4s 378us/step - loss: 6.9073e-05 - sparse_categorical_accuracy: 0.6303 - val_loss: 6.7627e-05 - val_sparse_categorical_accuracy: 0.6607\n",
      "Epoch 10/20\n",
      "10473/10473 [==============================] - 4s 388us/step - loss: 6.7613e-05 - sparse_categorical_accuracy: 0.6417 - val_loss: 6.5618e-05 - val_sparse_categorical_accuracy: 0.6710\n",
      "Epoch 11/20\n",
      "10473/10473 [==============================] - 4s 396us/step - loss: 6.5537e-05 - sparse_categorical_accuracy: 0.6570 - val_loss: 6.4236e-05 - val_sparse_categorical_accuracy: 0.6761\n",
      "Epoch 12/20\n",
      "10473/10473 [==============================] - 5s 446us/step - loss: 6.3416e-05 - sparse_categorical_accuracy: 0.6744 - val_loss: 6.2604e-05 - val_sparse_categorical_accuracy: 0.6916\n",
      "Epoch 13/20\n",
      "10473/10473 [==============================] - 4s 387us/step - loss: 6.1456e-05 - sparse_categorical_accuracy: 0.6833 - val_loss: 6.0984e-05 - val_sparse_categorical_accuracy: 0.7036\n",
      "Epoch 14/20\n",
      "10473/10473 [==============================] - 4s 386us/step - loss: 5.9239e-05 - sparse_categorical_accuracy: 0.7001 - val_loss: 5.9603e-05 - val_sparse_categorical_accuracy: 0.7045\n",
      "Epoch 15/20\n",
      "10473/10473 [==============================] - 4s 389us/step - loss: 5.7519e-05 - sparse_categorical_accuracy: 0.7100 - val_loss: 5.8350e-05 - val_sparse_categorical_accuracy: 0.7079\n",
      "Epoch 16/20\n",
      "10473/10473 [==============================] - 4s 386us/step - loss: 5.4817e-05 - sparse_categorical_accuracy: 0.7316 - val_loss: 5.7196e-05 - val_sparse_categorical_accuracy: 0.7191\n",
      "Epoch 17/20\n",
      "10473/10473 [==============================] - 5s 455us/step - loss: 5.3624e-05 - sparse_categorical_accuracy: 0.7342 - val_loss: 5.5415e-05 - val_sparse_categorical_accuracy: 0.7259\n",
      "Epoch 18/20\n",
      "10473/10473 [==============================] - 4s 392us/step - loss: 5.1031e-05 - sparse_categorical_accuracy: 0.7523 - val_loss: 5.3915e-05 - val_sparse_categorical_accuracy: 0.7311\n",
      "Epoch 19/20\n",
      "10473/10473 [==============================] - 4s 383us/step - loss: 4.9518e-05 - sparse_categorical_accuracy: 0.7545 - val_loss: 5.2632e-05 - val_sparse_categorical_accuracy: 0.7380\n",
      "Epoch 20/20\n",
      "10473/10473 [==============================] - 4s 388us/step - loss: 4.7191e-05 - sparse_categorical_accuracy: 0.7751 - val_loss: 5.1608e-05 - val_sparse_categorical_accuracy: 0.7380\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "dropout = 0.3\n",
    "hidden_1 = 128\n",
    "counting_dense_1 = 256\n",
    "counting_dense_2 = 64\n",
    "\n",
    "sel_cls_list = [[1, 2, 3, 4]]          #[2, 4], [1, 2, 3], [1, 2, 3, 4]\n",
    "model_type = 'lstm'\n",
    "data_mode = 'amp'\n",
    "model_dir = '/scratch/sk7898/pedbike/models'\n",
    "train_iter = 5\n",
    "               \n",
    "X_train, X_test, y_train, y_test = get_data(sel_cls=sel_cls, data_mode='amp')\n",
    "weights = [1/len(X_train) for i in range(len(X_train))]\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "for sel_cls in sel_cls_list:\n",
    "    if model_type == 'conv':\n",
    "        X_train = X_train[:, :, :, np.newaxis]\n",
    "        X_test = X_test[:, :, :, np.newaxis]\n",
    "\n",
    "        model = build_conv2d_fft_model(filters_1,\n",
    "                                       counting_dense_1,\n",
    "                                       counting_dense_2,\n",
    "                                       dropout=dropout,\n",
    "                                       optimizer=optimizer,\n",
    "                                       n_classes=len(sel_cls),\n",
    "                                       input_shape=(5, 256, 1))\n",
    "    else:\n",
    "        model = build_lstm_fft_model(hidden_1,\n",
    "                                     counting_dense_1,\n",
    "                                     counting_dense_2,\n",
    "                                     dropout=dropout,\n",
    "                                     optimizer=optimizer,\n",
    "                                     input_shape=(5, 256),\n",
    "                                     n_classes=len(sel_cls))    \n",
    "\n",
    "    for i in range(train_iter): \n",
    "        H_train = model.fit(x=X_train,\n",
    "                            y=y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_split=0.1,\n",
    "                            epochs=epochs,\n",
    "                            shuffle=True,\n",
    "                            sample_weight=np.array(weights))\n",
    "\n",
    "        pred = model.predict(X_train)\n",
    "        cls_pred = np.argmax(pred, axis = 1)\n",
    "        correct = np.where(cls_pred == y_train, 1, 0)\n",
    "        incorrect = np.where(cls_pred != y_train, 1, 0)\n",
    "\n",
    "        # Calculate the misclassification rate and accuracy\n",
    "        accuracy = sum(correct)/len(correct)\n",
    "        misclassification = sum(incorrect)/len(incorrect)\n",
    "\n",
    "        err = np.sum(weights * misclassification)/np.sum(weights)\n",
    "        alpha = np.log((1-err)/err)\n",
    "\n",
    "        # Update the weights wi --> These updated weights are used in the sample_weight parameter \n",
    "        weights *= np.exp(alpha * misclassification)\n",
    "\n",
    "    predictions = model.evaluate(x=X_test, y=y_test)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
