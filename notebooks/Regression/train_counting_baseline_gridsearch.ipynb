{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import types\n",
    "import random\n",
    "import argparse\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-tbs'], dest='tbs', nargs=None, const=None, default=64, type=<class 'int'>, choices=None, help='Number of samples per training batch', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Arguments')\n",
    "parser.add_argument('-base', type=str, default='/mnt/6b93b438-a3d4-40d2-9f3d-d8cdbb850183/Research/'\\\n",
    "                                                'Deep_Learning_Radar/Data/Counting',\\\n",
    "                                                help='Base location of data')\n",
    "\n",
    "parser.add_argument('-datatype', type=str, default='stft', help='Data type: \\'stft\\' for spectrogram,' \\\n",
    "                                                                ' \\'time\\' for raw time series)')\n",
    "\n",
    "parser.add_argument('-cv', type=str, default=None, help='Cross validation folds for GridSearch')\n",
    "parser.add_argument('-mt', type=str, default='conv1d_stft', help='type of embedding model')\n",
    "parser.add_argument('-window', type=int, default=256, help='Window length for embeddings to be extracted')\n",
    "parser.add_argument('-outdir', type=str, default=None, help='Output folder for model to be saved')\n",
    "#parser.add_argument('-epochs', type=int, default=10, help='Number of training epochs')\n",
    "#parser.add_argument('-lr', type=float, default=1e-4, help='Optimization learning rate')\n",
    "parser.add_argument('-tbs', type=int, default=64, help='Number of samples per training batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasGeneratorRegressor(KerasRegressor):\n",
    "    \"\"\"\n",
    "    Add fit_generator to KerasClassifier to get batches of sequences with similar length.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, **kwargs):    \n",
    "        # taken from keras.wrappers.scikit_learn.KerasClassifier.fit ###################################################\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType) and not isinstance(self.build_fn, types.MethodType):\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "\n",
    "        loss_name = self.model.loss\n",
    "        if hasattr(loss_name, '__name__'):\n",
    "            loss_name = loss_name.__name__\n",
    "        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n",
    "            y = to_categorical(y)\n",
    "        \n",
    "        ################################################################################################################\n",
    "        model_dict = self.filter_sk_params(self.build_fn)\n",
    "        model_str = '|'.join('{!s}={!s}'.format(key,val) for (key,val) in model_dict.items())\n",
    "        \n",
    "        n_bins = kwargs['n_bins']\n",
    "        output_path = kwargs['output_path']\n",
    "        epochs = self.sk_params['epochs']\n",
    "        output_path = os.path.join(kwargs['output_path'], model_str)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        #Callbacks for the training\n",
    "        early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, min_delta=1e-4, verbose=5, mode=\"auto\")\n",
    "        reduce_LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)        \n",
    "        #previous model naming convention: model_str+'.{epoch:02d}-{val_loss:.5f}.h5'\n",
    "        model_checkpoint = ModelCheckpoint(os.path.join(output_path, 'best_val_loss_model.h5'),\\\n",
    "                                           monitor=\"val_loss\", verbose=5, save_best_only=True, mode=\"auto\")\n",
    "        callbacks = [early_stopping, reduce_LR, model_checkpoint]\n",
    "        \n",
    "        if 'x_val' in kwargs and 'y_val' in kwargs:\n",
    "            x_train = X\n",
    "            y_train = y\n",
    "            x_val = kwargs['x_val']\n",
    "            y_val = kwargs['y_val']\n",
    "            seqs_train = kwargs['seq_lengths']\n",
    "        elif 'val_len' in kwargs:\n",
    "            val_size = kwargs['val_len'] \n",
    "            seqs_train = kwargs['seq_lengths']\n",
    "            x_train, x_val, y_train, y_val, seqs_train, seqs_val = train_test_split(X, y, seqs_train, test_size=val_size)\n",
    "        else:\n",
    "            raise ValueError('No validation data provided!')\n",
    "            \n",
    "        train_gen = train_generator(n_bins, x_train, y_train, seq_lengths=seqs_train, padding=True, padding_value=0.0)\n",
    "        val_gen = val_generator(x_val, y_val)\n",
    "\n",
    "        #for i, item in enumerate(train_gen):\n",
    "        #    print(np.array(item[0]).shape)\n",
    "            \n",
    "        self.__history = self.model.fit_generator(train_gen, validation_data=val_gen, \\\n",
    "                                                  validation_steps=len(val_gen),\\\n",
    "                                                  steps_per_epoch=len(train_gen),\\\n",
    "                                                  epochs=epochs,\\\n",
    "                                                  callbacks=callbacks)\n",
    "        return self.__history\n",
    "\n",
    "    def score(self, x, y, **kwargs):\n",
    "        \"\"\"Returns the mean loss on the given test data and labels.\n",
    "        # Arguments\n",
    "            x: array-like, shape `(n_samples, n_features)`\n",
    "                Test samples where `n_samples` is the number of samples\n",
    "                and `n_features` is the number of features.\n",
    "            y: array-like, shape `(n_samples,)`\n",
    "                True labels for `x`.\n",
    "            **kwargs: dictionary arguments\n",
    "                Legal arguments are the arguments of `Sequential.evaluate`.\n",
    "        # Returns\n",
    "            score: float\n",
    "                Mean accuracy of predictions on `x` wrt. `y`.\n",
    "        \"\"\"\n",
    "        #kwargs = self.filter_sk_params(Sequential.evaluate_generator, kwargs)\n",
    "        test_gen = val_generator(x, y)\n",
    "        loss = self.model.evaluate_generator(test_gen, steps=len(test_gen))\n",
    "        if isinstance(loss, list):\n",
    "            return -loss[0]\n",
    "        return -loss\n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return self.__history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_embedding_model(filters_1, filters_2, conv_dropout=None, reshape=None):\n",
    "    global input_shape\n",
    "    model = Sequential()\n",
    "    if reshape:\n",
    "        model.add(Reshape(reshape, input_shape=input_shape))\n",
    "        model.add(Conv1D(filters=filters_1, kernel_size=4, activation='relu'))\n",
    "    else:\n",
    "        model.add(Conv1D(filters=filters_1, kernel_size=4, activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv1D(filters=filters_2, kernel_size=2, activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    return model\n",
    "\n",
    "def lstm_embedding_model(hidden_1, hidden_2, num_layers=1, reshape=None):\n",
    "    global input_shape\n",
    "    model = Sequential()\n",
    "    if reshape:\n",
    "        model.add(Reshape(reshape, input_shape=input_shape))\n",
    "        model.add(LSTM(hidden_1, return_sequences=True, name='embedding_lstm_1'))\n",
    "    else:\n",
    "        model.add(LSTM(hidden_1, return_sequences=True, input_shape=input_shape, name='embedding_lstm_1'))\n",
    "    if num_layers == 2:\n",
    "        model.add(LSTM(hidden_2, return_sequences=True, input_shape=input_shape, name='embedding_lstm_2'))\n",
    "    return model\n",
    "\n",
    "def lstm_counting_model(model, counting_hidden_1,\\\n",
    "                        counting_dense_1, counting_dense_2, kernel_initializer='normal',\\\n",
    "                        optimizer=None, learning_rate=0.001, dropout=None):\n",
    "    \n",
    "    if optimizer == 'adam' or optimizer is None:\n",
    "        adam = keras.optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    model.add(Masking(mask_value=0.0, name='mask'))\n",
    "    model.add(LSTM(counting_hidden_1, return_sequences=False, name='counting_lstm_1'))\n",
    "    model.add(Dense(counting_dense_1, activation='relu', kernel_initializer=kernel_initializer, name='counting_dense_1'))\n",
    "    model.add(Dense(counting_dense_2, activation='relu', kernel_initializer=kernel_initializer, name='counting_dense_2'))\n",
    "    model.add(Dense(1, kernel_initializer=kernel_initializer, name='output'))\n",
    "    model.add(Activation('linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_stft_keys():\n",
    "    epochs = [1] #[10, 20, 30]\n",
    "    lr = [1e-2] #[1e-3, 1e-4, 1e-5]\n",
    "    optimizers = ['adam']\n",
    "    dropout = [0.2] #[0.2, 0.4]\n",
    "    hidden_1 = [32] #[32, 64]\n",
    "    hidden_2 = [32] #[32, 64]\n",
    "    counting_hidden_1 = [32] #[32, 64]\n",
    "    counting_dense_1 = [32] #[32, 64, 128]\n",
    "    counting_dense_2 = [64] #[32, 64]\n",
    "    param_grid = dict(epochs=epochs, hidden_1=hidden_1, hidden_2=hidden_2,\\\n",
    "                      counting_hidden_1=counting_hidden_1,\\\n",
    "                      counting_dense_1=counting_dense_1, counting_dense_2=counting_dense_2,\\\n",
    "                      learning_rate=lr, optimizer=optimizers, dropout=dropout)\n",
    "    return param_grid\n",
    "\n",
    "def conv1d_stft_keys():\n",
    "    epochs = [1]\n",
    "    lr = [1e-2]\n",
    "    optimizers = ['adam'] \n",
    "    conv_dropout= [0.2]\n",
    "    dropout = [0.2]\n",
    "    filters_1 = [64]\n",
    "    filters_2 = [64]\n",
    "    counting_hidden_1 = [32] \n",
    "    counting_dense_1 = [32]\n",
    "    counting_dense_2 = [32]\n",
    "    param_grid = dict(epochs=epochs, filters_1=filters_1, filters_2=filters_2,\\\n",
    "                      conv_dropout=conv_dropout, counting_hidden_1=counting_hidden_1,\\\n",
    "                      counting_dense_1=counting_dense_1, counting_dense_2=counting_dense_2,\\\n",
    "                      learning_rate=lr, optimizer=optimizers, dropout=dropout)\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_stft_model(hidden_1, hidden_2, counting_hidden_1,\\\n",
    "                          counting_dense_1, counting_dense_2,\\\n",
    "                          kernel_initializer='normal',\\\n",
    "                          learning_rate=1e-2, optimizer='adam', dropout=None):\n",
    "    \n",
    "    model = lstm_embedding_model(hidden_1, hidden_2)\n",
    "    counting_model = lstm_counting_model(model, counting_hidden_1,\\\n",
    "                                         counting_dense_1, counting_dense_2,\\\n",
    "                                         kernel_initializer=kernel_initializer,\\\n",
    "                                         optimizer=optimizer, learning_rate=learning_rate, dropout=dropout)\n",
    "    return counting_model\n",
    "    \n",
    "def build_lstm_time_model(hidden_1, hidden_2, counting_hidden_1,\\\n",
    "                          counting_dense_1, counting_dense_2,\\\n",
    "                          kernel_initializer='normal',\\\n",
    "                          learning_rate=1e-2, optimizer='adam', dropout=None):\n",
    "    \n",
    "    model = lstm_embedding_model(hidden_1, hidden_2, reshape=(-1, 2))\n",
    "    counting_model = lstm_counting_model(model, counting_hidden_1,\\\n",
    "                                         counting_dense_1, counting_dense_2,\\\n",
    "                                         kernel_initializer=kernel_initializer,\\\n",
    "                                         optimizer=optimizer, learning_rate=learning_rate, dropout=dropout)\n",
    "    return counting_model\n",
    "\n",
    "def build_conv1d_stft_model(filters_1, filters_2, counting_hidden_1, counting_dense_1, counting_dense_2,\\\n",
    "                            kernel_initializer='normal', conv_dropout=None,\\\n",
    "                            learning_rate=1e-2, optimizer='adam', dropout=None):\n",
    "    \n",
    "    model = conv1d_embedding_model(filters_1, filters_2, conv_dropout=conv_dropout)\n",
    "    counting_model = lstm_counting_model(model, counting_hidden_1,\\\n",
    "                                         counting_dense_1, counting_dense_2,\\\n",
    "                                         kernel_initializer=kernel_initializer,\\\n",
    "                                         optimizer=optimizer, learning_rate=learning_rate, dropout=dropout)\n",
    "    return counting_model\n",
    "    \n",
    "def build_conv1d_time_model(filters_1, filters_2, counting_hidden_1, counting_dense_1, counting_dense_2,\\\n",
    "                            kernel_initializer='normal', conv_dropout=None,\\\n",
    "                            learning_rate=1e-2, optimizer='adam', dropout=None):\n",
    "    \n",
    "    model = conv1d_embedding_model(filters_1, filters_2, conv_dropout=conv_dropout, reshape=(-1, 2))\n",
    "    counting_model = lstm_counting_model(model, counting_hidden_1, counting_dense_1, counting_dense_2,\\\n",
    "                                         kernel_initializer=kernel_initializer,\\\n",
    "                                         optimizer=optimizer, learning_rate=learning_rate, dropout=dropout)\n",
    "    return counting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken fom https://github.com/davidsbatista/machine-learning-notebooks/blob/master/hyperparameter-across-models.ipynb\n",
    "class EstimatorSelectionHelper:\n",
    "    \n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "    \n",
    "    def fit(self, X, y, **grid_kwargs):\n",
    "        for key in self.keys:\n",
    "            print('Running GridSearchCV for %s.' % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            grid_search = GridSearchCV(model, params, n_jobs=1, cv=2)\n",
    "            grid_search.fit(X, y, **grid_kwargs)\n",
    "            self.grid_searches[key] = grid_search\n",
    "        print('Done.')\n",
    "    \n",
    "    def score_summary(self, sort_by='mean_test_score'):\n",
    "        frames = []\n",
    "        for name, grid_search in self.grid_searches.items():\n",
    "            frame = pd.DataFrame(grid_search.cv_results_)\n",
    "            frame = frame.filter(regex='^(?!.*param_).*$')\n",
    "            frame['estimator'] = len(frame)*[name]\n",
    "            frames.append(frame)\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "        df = df.sort_values([sort_by], ascending=False)\n",
    "        df = df.reset_index()\n",
    "        df = df.drop(['rank_test_score', 'index'], 1)\n",
    "        \n",
    "        columns = df.columns.tolist()\n",
    "        columns.remove('estimator')\n",
    "        columns = ['estimator']+columns\n",
    "        df = df[columns]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=parser.parse_args()\n",
    "\n",
    "# data_type = args.datatype\n",
    "# base_path = args.base\n",
    "# model_type = args.mt\n",
    "# window = int(args.window)\n",
    "# cv = int(args.cv)\n",
    "# batch_size, epochs = args.tbs, args.epochs\n",
    "\n",
    "if True:\n",
    "    data_type = 'time'\n",
    "    window = 256\n",
    "    cv = 2\n",
    "    model_type = 'lstm_time'\n",
    "    base_path = '/scratch/sk7898/pedbike/window_256'\n",
    "    batch_size, epochs = 64, 10\n",
    "\n",
    "if data_type == 'stft':\n",
    "    fileloc = os.path.join(base_path, 'downstream_stft')\n",
    "elif data_type == 'time':\n",
    "    fileloc = os.path.join(base_path, 'downstream_time')\n",
    "else:\n",
    "    raise ValueError('Only stft/time are valid data types')\n",
    "    \n",
    "x_train, x_val, x_test, y_train, y_val, y_test, seqs_train, seqs_val, seqs_test = get_data(fileloc)\n",
    "n_bins = int(len(seqs_train)/batch_size)\n",
    "\n",
    "if cv:\n",
    "    x_train = np.array(x_train.tolist() + x_val.tolist())\n",
    "    y_train = np.array(y_train.tolist() + y_val.tolist())\n",
    "    seqs_train = np.array(seqs_train.tolist() + seqs_val.tolist())\n",
    "    \n",
    "assert x_train.shape[0] == y_train.shape[0] == seqs_train.shape[0]    \n",
    "\n",
    "n_timesteps, n_features = None, window*2\n",
    "input_shape=(n_timesteps, n_features)\n",
    "\n",
    "output_path = os.path.join('/scratch/sk7898/radar_counting/models', model_type)\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "models = {\n",
    "    'lstm_stft': build_lstm_stft_model,\n",
    "    'lstm_time': build_lstm_time_model,\n",
    "    'conv1d_stft': build_conv1d_stft_model,\n",
    "    'conv1d_time': build_conv1d_time_model\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'lstm_stft': lstm_stft_keys(),\n",
    "    'lstm_time': lstm_stft_keys(),\n",
    "    'conv1d_stft': conv1d_stft_keys(),\n",
    "    'conv1d_time': conv1d_stft_keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1355: calling reduce_any (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-12/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/1\n",
      "11/12 [==========================>...] - ETA: 6s - loss: 0.4133 - mean_absolute_error: 0.5487 Epoch 00001: val_loss improved from inf to 0.26481, saving model to /scratch/sk7898/radar_counting/models/lstm_time/counting_dense_1=32|counting_dense_2=64|counting_hidden_1=32|dropout=0.2|hidden_1=32|hidden_2=32|learning_rate=0.01|optimizer=adam/best_val_loss_model.h5\n",
      "12/12 [==============================] - 249s 21s/step - loss: 0.3932 - mean_absolute_error: 0.5375 - val_loss: 0.2648 - val_mean_absolute_error: 0.5072\n"
     ]
    }
   ],
   "source": [
    "model = KerasGeneratorRegressor(build_fn=models[model_type], verbose=1)\n",
    "if cv:\n",
    "    grid = GridSearchCV(estimator=model, param_grid=params[model_type], n_jobs=1, cv=cv)\n",
    "    grid_result = grid.fit(x_train, y_train, n_bins=n_bins, val_len=x_val.shape[0],\\\n",
    "                           seq_lengths=seqs_train, output_path=output_path)\n",
    "else:\n",
    "    grid = GridSearchCV(estimator=model, param_grid=params[model_type], n_jobs=1, cv=[(slice(None), slice(None))])\n",
    "    grid_result = grid.fit(x_train, y_train, x_val=x_val, y_val=y_val, n_bins=n_bins,\\\n",
    "                           seq_lengths=seqs_train, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{} {} with: {}\".format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by='mean_test_score'\n",
    "frame = pd.DataFrame(grid_result.cv_results_)\n",
    "df = frame.filter(regex='^(?!.*param_).*$')\n",
    "df = df.sort_values([sort_by], ascending=False)\n",
    "df = df.reset_index()\n",
    "df = df.drop(['rank_test_score', 'index'], 1)\n",
    "df.to_csv(os.path.join(output_path, 'summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(output_path, 'summary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = grid.score(x_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
