{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import keras\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import LSTM, Dense, Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(lst1, lst2): \n",
    "    return [value for value in lst1 if value in lst2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_data(X_train, X_test, data_shape):\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_train = scaler.transform(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "    X_train = X_train.reshape(X_train.shape[0], data_shape[1], data_shape[2])\n",
    "    X_test = X_test.reshape(X_test.shape[0], data_shape[1], data_shape[2])\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_cls(labels, sel_cls):\n",
    "    \n",
    "    for i, lbl in enumerate(sel_cls):\n",
    "        labels[labels == i] = lbl\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbls_for_cls(labels, lbls_list=None):\n",
    "    new_labels = [i for i in range(len(lbls_list))]\n",
    "    for i, lbl in enumerate(lbls_list):\n",
    "        labels[labels == lbl] = new_labels[i]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_str(sel_cls, hidden_1=64, data_mode='amp', win_len=512):\n",
    "    model_str = ''\n",
    "    for cls in sel_cls:\n",
    "        model_str += str(cls) + '_'\n",
    "    \n",
    "    model_str += data_mode + '_' + str(win_len) + '_hidden_' + str(hidden_1) \n",
    "    #model_str += '_' + str(int(validation_split*100)) if validation_split else '_None' \n",
    "    return model_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_data(data, labels, sel_cls):\n",
    "    labels_idx = []\n",
    "    for cls in sel_cls:\n",
    "        labels_idx += np.argwhere(labels == cls).flatten().tolist()\n",
    "\n",
    "    sel_labels = [labels[idx] for idx in labels_idx]\n",
    "    sel_data = [data[idx] for idx in labels_idx]\n",
    "    labels = np.array(sel_labels)\n",
    "    data = np.array(sel_data)\n",
    "    data = data.astype(np.float32) \n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downsampled_points(data, labels, target_cls=[1, 2, 3]):\n",
    "    \n",
    "    subset_idx = {}\n",
    "    downsampled_idx = []\n",
    "    \n",
    "    model_dir = '/scratch/sk7898/pedbike/models/lstm/'\n",
    "    cls_str_list = ['1_2', '1_3', '2_3']\n",
    "    sel_cls_list = [[1, 2], [1, 3], [2, 3]]\n",
    "     \n",
    "    for idx, (cls_str, sel_cls) in enumerate(zip(cls_str_list, sel_cls_list)):\n",
    "        X, y = get_cls_data(data, labels, sel_cls)\n",
    "\n",
    "        model_str = os.path.join(cls_str + '_amp_512_hidden_128/best_model.h5')\n",
    "        model_path = os.path.join(model_dir, model_str)\n",
    "\n",
    "        # Load the model to predict the count class\n",
    "        model = load_model(model_path) \n",
    "        pred = model.predict(x=X)\n",
    "        cls_pred = np.argmax(pred, axis = 1)\n",
    "        y_pred = get_true_cls(cls_pred, sel_cls)\n",
    "        \n",
    "        for cls in sel_cls:\n",
    "            if cls not in subset_idx.keys():\n",
    "                subset_idx[cls] = []\n",
    "                subset_idx[cls] = [i for i, (x, y) in enumerate(zip(y, y_pred)) if x == y]\n",
    "            else:\n",
    "                c_idx = [i for i, (x, y) in enumerate(zip(y, y_pred)) if x == y]\n",
    "                subset_idx[cls] = get_intersection(subset_idx[cls], c_idx)\n",
    "                \n",
    "    for key in subset_idx.keys():\n",
    "        downsampled_idx += subset_idx[key]\n",
    "    \n",
    "    sel_labels = [labels[idx] for idx in downsampled_idx]\n",
    "    sel_data = [data[idx] for idx in downsampled_idx]\n",
    "    labels = np.array(sel_labels)\n",
    "    data = np.array(sel_data).astype(np.float32) \n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sel_cls,\n",
    "             data_mode='amp', \n",
    "             task_type='cls',\n",
    "             downsample=True,\n",
    "             scaling=True):\n",
    "    \n",
    "    data_dir = '/scratch/sk7898/pedbike/fft_data'\n",
    "    data_path = os.path.join(data_dir, 'Data_win_fft.npy')\n",
    "    labels_path = os.path.join(data_dir, 'label_win_fft.npy')\n",
    "    seqs_path = os.path.join(data_dir, 'seqs_fft.npy')\n",
    "    data = np.load(data_path, allow_pickle=True) #shape: (18642, 256, 5)\n",
    "    labels = np.load(labels_path, allow_pickle=True) #shape: (18642,)\n",
    "\n",
    "    n_data = data.swapaxes(1, 2)\n",
    "    amp_data = np.absolute(n_data)\n",
    "    phase_data = np.angle(n_data)\n",
    "    power_data = np.absolute(n_data)**2\n",
    "    real_data = np.real(n_data)\n",
    "    imag_data = np.imag(n_data)\n",
    "    \n",
    "    if data_mode == 'amp':\n",
    "        data = amp_data\n",
    "    elif data_mode == 'phase':\n",
    "        data = phase_data\n",
    "    elif data_mode == 'power':\n",
    "        data == power_data\n",
    "\n",
    "    data, labels = get_cls_data(data, labels, sel_cls)\n",
    "\n",
    "    if downsample:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                            labels,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=42)\n",
    "        if scaling:\n",
    "            X_train, X_test = get_scaled_data(X_train, X_test, data_shape=n_data.shape)\n",
    "        \n",
    "        print('X_train Before Downsampling: ', X_train.shape)\n",
    "        X_train, y_train = get_downsampled_points(X_train,\n",
    "                                                  y_train,\n",
    "                                                  target_cls=sel_cls)\n",
    "                \n",
    "        print('X_train After Downsampling: ', X_train.shape)\n",
    "        y_train = lbls_for_cls(y_train, lbls_list=sel_cls)\n",
    "        y_test = lbls_for_cls(y_test, lbls_list=sel_cls)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    else:\n",
    "        labels = lbls_for_cls(labels, lbls_list=sel_cls)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                            labels,\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=42)\n",
    "    \n",
    "        if scaling:\n",
    "            X_train, X_test = get_scaled_data(X_train, X_test, data_shape=n_data.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_fft_model(hidden_1,\n",
    "                         counting_dense_1,\n",
    "                         counting_dense_2,\n",
    "                         kernel_initializer='normal',\n",
    "                         dropout_1=None,\n",
    "                         dropout_2=None,\n",
    "                         optimizer=None,\n",
    "                         input_shape=(5, 256),\n",
    "                         n_classes=2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_1, return_sequences=False, input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(counting_dense_1, activation='relu', name='counting_dense_1'))\n",
    "    model.add(Dropout(dropout_1))\n",
    "    model.add(Dense(counting_dense_2, activation='relu', name='counting_dense_2'))\n",
    "    model.add(Dropout(dropout_2))\n",
    "    model.add(Dense(n_classes, activation='softmax', name='output'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['sparse_categorical_accuracy', 'mae'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv2d_fft_model(filters_1,\n",
    "                           counting_dense_1,\n",
    "                           counting_dense_2,\n",
    "                           kernel_initializer='normal',\n",
    "                           dropout=None,\n",
    "                           optimizer=None,\n",
    "                           input_shape=(5, 256, 1),\n",
    "                           task_type=None,\n",
    "                           n_classes=2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters_1, kernel_size=(2, 8), strides=(1, 8), data_format='channels_last', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(counting_dense_1, activation='relu', name='counting_dense_1'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(counting_dense_2, activation='relu', name='counting_dense_2'))\n",
    "    model.add(Dense(n_classes, activation='softmax', name='output'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['sparse_categorical_accuracy', 'mae'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10473 samples, validate on 1164 samples\n",
      "Epoch 1/20\n",
      "10473/10473 [==============================] - 3s 261us/step - loss: 1.0968 - sparse_categorical_accuracy: 0.4386 - mae: 0.9844 - val_loss: 0.9781 - val_sparse_categorical_accuracy: 0.5464 - val_mae: 0.9762\n",
      "Epoch 2/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.9608 - sparse_categorical_accuracy: 0.5334 - mae: 0.9844 - val_loss: 0.8845 - val_sparse_categorical_accuracy: 0.5885 - val_mae: 0.9762\n",
      "Epoch 3/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.8970 - sparse_categorical_accuracy: 0.5740 - mae: 0.9844 - val_loss: 0.8371 - val_sparse_categorical_accuracy: 0.6246 - val_mae: 0.9762\n",
      "Epoch 4/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.8529 - sparse_categorical_accuracy: 0.5999 - mae: 0.9844 - val_loss: 0.8083 - val_sparse_categorical_accuracy: 0.6375 - val_mae: 0.9762\n",
      "Epoch 5/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.8149 - sparse_categorical_accuracy: 0.6304 - mae: 0.9844 - val_loss: 0.7811 - val_sparse_categorical_accuracy: 0.6503 - val_mae: 0.9762\n",
      "Epoch 6/20\n",
      "10473/10473 [==============================] - 2s 198us/step - loss: 0.7787 - sparse_categorical_accuracy: 0.6474 - mae: 0.9844 - val_loss: 0.7577 - val_sparse_categorical_accuracy: 0.6667 - val_mae: 0.9762\n",
      "Epoch 7/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.7412 - sparse_categorical_accuracy: 0.6726 - mae: 0.9844 - val_loss: 0.7332 - val_sparse_categorical_accuracy: 0.6847 - val_mae: 0.9762\n",
      "Epoch 8/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.7045 - sparse_categorical_accuracy: 0.6915 - mae: 0.9844 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.6959 - val_mae: 0.9762\n",
      "Epoch 9/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.6713 - sparse_categorical_accuracy: 0.7100 - mae: 0.9844 - val_loss: 0.6881 - val_sparse_categorical_accuracy: 0.7139 - val_mae: 0.9762\n",
      "Epoch 10/20\n",
      "10473/10473 [==============================] - 2s 197us/step - loss: 0.6351 - sparse_categorical_accuracy: 0.7317 - mae: 0.9844 - val_loss: 0.6734 - val_sparse_categorical_accuracy: 0.7216 - val_mae: 0.9762\n",
      "Epoch 11/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.6049 - sparse_categorical_accuracy: 0.7455 - mae: 0.9844 - val_loss: 0.6558 - val_sparse_categorical_accuracy: 0.7156 - val_mae: 0.9762\n",
      "Epoch 12/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.5722 - sparse_categorical_accuracy: 0.7629 - mae: 0.9844 - val_loss: 0.6389 - val_sparse_categorical_accuracy: 0.7277 - val_mae: 0.9762\n",
      "Epoch 13/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.5385 - sparse_categorical_accuracy: 0.7784 - mae: 0.9844 - val_loss: 0.6180 - val_sparse_categorical_accuracy: 0.7500 - val_mae: 0.9762\n",
      "Epoch 14/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.5057 - sparse_categorical_accuracy: 0.7929 - mae: 0.9844 - val_loss: 0.6058 - val_sparse_categorical_accuracy: 0.7380 - val_mae: 0.9762\n",
      "Epoch 15/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.4733 - sparse_categorical_accuracy: 0.8075 - mae: 0.9844 - val_loss: 0.5986 - val_sparse_categorical_accuracy: 0.7414 - val_mae: 0.9762\n",
      "Epoch 16/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.4406 - sparse_categorical_accuracy: 0.8198 - mae: 0.9844 - val_loss: 0.5713 - val_sparse_categorical_accuracy: 0.7526 - val_mae: 0.9762\n",
      "Epoch 17/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.4086 - sparse_categorical_accuracy: 0.8375 - mae: 0.9844 - val_loss: 0.5566 - val_sparse_categorical_accuracy: 0.7698 - val_mae: 0.9762\n",
      "Epoch 18/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8466 - mae: 0.9844 - val_loss: 0.5593 - val_sparse_categorical_accuracy: 0.7603 - val_mae: 0.9762\n",
      "Epoch 19/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.3570 - sparse_categorical_accuracy: 0.8600 - mae: 0.9844 - val_loss: 0.5586 - val_sparse_categorical_accuracy: 0.7689 - val_mae: 0.9762\n",
      "Epoch 20/20\n",
      "10473/10473 [==============================] - 2s 196us/step - loss: 0.3344 - sparse_categorical_accuracy: 0.8686 - mae: 0.9844 - val_loss: 0.5405 - val_sparse_categorical_accuracy: 0.7887 - val_mae: 0.9762\n",
      "1294/1294 [==============================] - 0s 68us/step\n",
      "Accuracy: 0.7588871717453003 MAE: 0.28052550231839257\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "dropout_1 = 0.3\n",
    "dropout_2 = 0.3\n",
    "hidden_1 = 128\n",
    "filters_1 = 64\n",
    "counting_dense_1 = 256\n",
    "counting_dense_2 = 64\n",
    "\n",
    "cls_list = [[1, 2, 3]] #[2, 4], [1, 2, 3], [1, 2, 3, 4]\n",
    "model_type = 'lstm'\n",
    "data_mode = 'amp'\n",
    "model_dir = '/scratch/sk7898/pedbike/models'\n",
    "\n",
    "for sel_cls in cls_list:\n",
    "    model_str = get_model_str(sel_cls, hidden_1=hidden_1, data_mode='amp', win_len=512)\n",
    "    model_path = os.path.join(model_dir, model_type, model_str)\n",
    "    \n",
    "    if not os.path.isdir(model_path):\n",
    "        os.makedirs(model_path)\n",
    "                \n",
    "    X_train, X_test, y_train, y_test = get_data(sel_cls=sel_cls, data_mode='amp', downsample=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    if model_type == 'conv':\n",
    "        X_train = X_train[:, :, :, np.newaxis]\n",
    "        X_test = X_test[:, :, :, np.newaxis]\n",
    "        \n",
    "#         print(X_train.shape) #(7110, 5, 256, 1)\n",
    "#         print(X_test.shape) #(791, 5, 256, 1)\n",
    "    \n",
    "        model = build_conv2d_fft_model(filters_1,\n",
    "                                       counting_dense_1,\n",
    "                                       counting_dense_2,\n",
    "                                       dropout_1=dropout_1,\n",
    "                                       optimizer=optimizer,\n",
    "                                       n_classes=len(sel_cls),\n",
    "                                       input_shape=(5, 256, 1))\n",
    "    else:\n",
    "        model = build_lstm_fft_model(hidden_1,\n",
    "                                     counting_dense_1,\n",
    "                                     counting_dense_2,\n",
    "                                     dropout_1=dropout_1,\n",
    "                                     dropout_2=dropout_2,\n",
    "                                     optimizer=optimizer,\n",
    "                                     input_shape=(5, 256),\n",
    "                                     n_classes=len(sel_cls))    \n",
    "    H_train = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=epochs,\n",
    "                        shuffle=True)\n",
    "    \n",
    "    \n",
    "    model.save(os.path.join(model_path, 'latest_model_downsample.h5'))\n",
    "    evaluations = model.evaluate(x=X_test, y=y_test)    \n",
    "    pred = model.predict(x=X_test)\n",
    "    cls_pred = np.argmax(pred, axis = 1)\n",
    "    mae = mean_absolute_error(y_test, cls_pred)\n",
    "    print('Accuracy: {} MAE: {}'.format(evaluations[1], mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
