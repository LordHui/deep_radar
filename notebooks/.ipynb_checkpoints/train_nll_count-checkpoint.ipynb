{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import types\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative binomial\n",
    "(copied from https://github.com/gokceneraslan/neuralnet_countmodels)\n",
    "\n",
    "The major limitation of Poisson distribution is that the mean and variance are equal and are controlled by a single parameter, $\\lambda$. This, however, is not realistic in many real world cases. [Negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) (NB) is another discrete distribution which can be used when mean and variance of the data are not equal. Although the classical textbook definition of NB is *the number of successes before a specified number of failures occur in i.i.d Bernoulli trials*, the alternative parameterization with mean ($\\mu$) and dispersion ($\\theta$) is more intuitive and useful. See these [lecture notes](https://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture6.htm) for a nice derivation of this parameterization. Here is the probability mass function according to this parameterization:\n",
    "\n",
    "$$ L_{nbinom}(x;\\mu,\\theta) = \\frac{\\Gamma(x+\\theta)}{\\Gamma(\\theta)\\Gamma(x+1)}\\left( \\frac{\\theta}{\\theta+\\mu}\\right)^\\theta\\left(\\frac{\\mu}{\\theta+\\mu}\\right)^x$$\n",
    "\n",
    "where parameters $\\mu$ and $\\theta$ represent the mean and dispersion. The negative log likelihood is:\n",
    "\n",
    "$$ NLL_{nbinom}(x;\\mu,\\theta) = -\\log\\Gamma(x+\\theta)+\\log\\Gamma(\\theta)+\\log\\Gamma(x+1)-\\theta\\left(\\log\\theta-\\log(\\theta+\\mu)\\right)-x\\left(\\log\\mu-\\log(\\theta+\\mu)\\right)$$\n",
    "\n",
    "Note that, this reduces to Poisson as $\\theta\\to\\infty$. Here we will use $\\theta^{-1}$ as the dispersion parameter since it is more convenient e.g. we can initialize $\\theta^{-1}$ to $0$. We can now implement NB loss in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(object):\n",
    "    def __init__(self, theta=None, theta_init=[0.0],\n",
    "                 scale_factor=1.0, scope='nbinom_loss/',\n",
    "                 debug=False, **theta_kwargs):\n",
    "        \n",
    "        # for numerical stability\n",
    "        self.eps = 1e-10\n",
    "        self.scale_factor = scale_factor\n",
    "        self.debug = debug\n",
    "        self.scope = scope\n",
    "           \n",
    "    def loss(self, y_true, y_pred, reduce=True):\n",
    "        scale_factor = self.scale_factor\n",
    "        eps = self.eps\n",
    "        \n",
    "        with tf.name_scope(self.scope):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred, tf.float32) * scale_factor\n",
    "            mean = y_pred[:,0]\n",
    "            theta = y_pred[:,1]        \n",
    "            \n",
    "            # Clip theta\n",
    "            theta = tf.minimum(theta, 1e6)\n",
    "\n",
    "            t1 = tf.lgamma(theta+eps) + tf.lgamma(y_true+1.0) - tf.lgamma(y_true+theta+eps)\n",
    "            t2 = (theta+y_true) * tf.log(1.0 + (mean/(theta+eps))) + (y_true * (tf.log(theta+eps) - tf.log(mean+eps)))    \n",
    "\n",
    "            if self.debug:\n",
    "                tf.summary.histogram('t1', t1)\n",
    "                tf.summary.histogram('t2', t2)\n",
    "\n",
    "            final = t1 + t2\n",
    "            \n",
    "            if reduce:\n",
    "                final = tf.reduce_mean(final)\n",
    "            \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_embedding_model(filters_1, filters_2, conv_dropout=None, reshape=None):\n",
    "    global input_shape\n",
    "    model = Sequential()\n",
    "    if reshape:\n",
    "        model.add(Reshape(reshape, input_shape=input_shape))\n",
    "        model.add(Conv1D(filters=filters_1, kernel_size=4, activation='relu'))\n",
    "    else:\n",
    "        model.add(Conv1D(filters=filters_1, kernel_size=4, activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv1D(filters=filters_2, kernel_size=2, activation='relu'))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    return model\n",
    "\n",
    "def lstm_embedding_model(hidden_1, hidden_2,\\\n",
    "                         num_layers=1, reshape=None):\n",
    "    global input_shape\n",
    "    model = Sequential()\n",
    "    if reshape:\n",
    "        model.add(Reshape(reshape, input_shape=input_shape))\n",
    "        model.add(LSTM(hidden_1, return_sequences=True))\n",
    "    else:\n",
    "        model.add(LSTM(hidden_1, return_sequences=True, input_shape=input_shape))\n",
    "    if num_layers == 2:\n",
    "        model.add(LSTM(hidden_2, return_sequences=True, input_shape=input_shape))\n",
    "    return model\n",
    "\n",
    "def lstm_counting_model(model, counting_hidden_1, counting_dense_1,\\\n",
    "                        counting_dense_2, kernel_initializer='normal',\\\n",
    "                        optimizer=None, learning_rate=0.001, dropout=None):\n",
    "    \n",
    "    if optimizer == 'adam' or optimizer is None:\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer == 'rms':\n",
    "        optimizer = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9)\n",
    "    \n",
    "    model.add(Masking(mask_value=0.0, name='mask'))\n",
    "    model.add(LSTM(counting_hidden_1, return_sequences=False, name='counting_lstm_1'))\n",
    "    model.add(Dense(counting_dense_1, activation='relu', kernel_initializer=kernel_initializer, name='counting_dense_1'))\n",
    "    model.add(Dense(counting_dense_2, activation='relu', kernel_initializer=kernel_initializer, name='counting_dense_2'))\n",
    "    model.add(Dense(2, kernel_initializer=kernel_initializer, name='output'))\n",
    "    model.add(Activation('softplus'))\n",
    "    model.compile(loss=NB().loss, optimizer=optimizer, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_time_model(hidden_1, hidden_2, counting_hidden_1,\\\n",
    "                          counting_dense_1, counting_dense_2,\\\n",
    "                          kernel_initializer='normal',\\\n",
    "                          learning_rate=1e-2, optimizer='adam', dropout=None):\n",
    "    \n",
    "    model = lstm_embedding_model(hidden_1, hidden_2, reshape=(-1, 2))\n",
    "    counting_model = lstm_counting_model(model, counting_hidden_1,\\\n",
    "                                         counting_dense_1, counting_dense_2,\\\n",
    "                                         kernel_initializer=kernel_initializer,\\\n",
    "                                         optimizer=optimizer, learning_rate=learning_rate, dropout=dropout)\n",
    "    return counting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    data_type = 'time'\n",
    "    window = 256\n",
    "    loss = 'nll'\n",
    "    model_type = 'lstm_time'\n",
    "    base_path = '/scratch/sk7898/pedbike/window_256'\n",
    "\n",
    "if data_type == 'stft':\n",
    "    fileloc = os.path.join(base_path, 'downstream_stft')\n",
    "elif data_type == 'time':\n",
    "    fileloc = os.path.join(base_path, 'downstream_time')\n",
    "else:\n",
    "    raise ValueError('Only stft/time are valid data types')\n",
    "    \n",
    "x_train, x_val, x_test, y_train, y_val, y_test, seqs_train, seqs_val, seqs_test = get_data(fileloc)\n",
    "n_bins = int(len(seqs_train)/batch_size)\n",
    "    \n",
    "assert x_train.shape[0] == y_train.shape[0] == seqs_train.shape[0]    \n",
    "\n",
    "n_timesteps, n_features = None, window*2\n",
    "input_shape=(n_timesteps, n_features)\n",
    "\n",
    "train_gen = train_generator(n_bins, x_train, y_train, seq_lengths=seqs_train, padding=True, padding_value=0.0)\n",
    "val_gen = val_generator(x_val, y_val)\n",
    "test_gen = val_generator(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11/12 [==========================>...] - ETA: 7s - loss: 2.6385 - mean_absolute_error: 1.6411 Epoch 00001: val_loss improved from inf to 1.53926, saving model to /scratch/sk7898/radar_counting/models/nll/lstm_time/20191220185953/best_val_loss_model.h5\n",
      "12/12 [==============================] - 85s 7s/step - loss: 2.6242 - mean_absolute_error: 1.6235 - val_loss: 1.5393 - val_mean_absolute_error: 0.3041\n",
      "Epoch 2/5\n",
      "11/12 [==========================>...] - ETA: 6s - loss: 2.5703 - mean_absolute_error: 1.5588 Epoch 00002: val_loss improved from 1.53926 to 1.53719, saving model to /scratch/sk7898/radar_counting/models/nll/lstm_time/20191220185953/best_val_loss_model.h5\n",
      "12/12 [==============================] - 82s 7s/step - loss: 2.6206 - mean_absolute_error: 1.6212 - val_loss: 1.5372 - val_mean_absolute_error: 0.3011\n",
      "Epoch 3/5\n",
      "11/12 [==========================>...] - ETA: 7s - loss: 2.6983 - mean_absolute_error: 1.7188 Epoch 00003: val_loss improved from 1.53719 to 1.53515, saving model to /scratch/sk7898/radar_counting/models/nll/lstm_time/20191220185953/best_val_loss_model.h5\n",
      "12/12 [==============================] - 82s 7s/step - loss: 2.6168 - mean_absolute_error: 1.6187 - val_loss: 1.5351 - val_mean_absolute_error: 0.2979\n",
      "Epoch 4/5\n",
      "11/12 [==========================>...] - ETA: 6s - loss: 2.5643 - mean_absolute_error: 1.5566 Epoch 00004: val_loss improved from 1.53515 to 1.53286, saving model to /scratch/sk7898/radar_counting/models/nll/lstm_time/20191220185953/best_val_loss_model.h5\n",
      "12/12 [==============================] - 82s 7s/step - loss: 2.6127 - mean_absolute_error: 1.6161 - val_loss: 1.5329 - val_mean_absolute_error: 0.2944\n",
      "Epoch 5/5\n",
      "11/12 [==========================>...] - ETA: 6s - loss: 2.5659 - mean_absolute_error: 1.5616 Epoch 00005: val_loss improved from 1.53286 to 1.53020, saving model to /scratch/sk7898/radar_counting/models/nll/lstm_time/20191220185953/best_val_loss_model.h5\n",
      "12/12 [==============================] - 82s 7s/step - loss: 2.6081 - mean_absolute_error: 1.6131 - val_loss: 1.5302 - val_mean_absolute_error: 0.2901\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 5\n",
    "lr = 1e-4\n",
    "optimizer = 'adam'\n",
    "dropout = 0.2\n",
    "hidden_1 = 32\n",
    "hidden_2 = 32\n",
    "counting_hidden_1 = 32\n",
    "counting_dense_1 = 32\n",
    "counting_dense_2 = 64\n",
    "\n",
    "output_path = os.path.join('/scratch/sk7898/radar_counting/models/' + loss, model_type,\\\n",
    "                           datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#Callbacks for the training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, min_delta=1e-4, verbose=5, mode='auto')\n",
    "reduce_LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)        \n",
    "model_checkpoint = ModelCheckpoint(os.path.join(output_path, 'best_val_loss_model.h5'),\\\n",
    "                                   monitor='val_loss', verbose=5, save_best_only=True, mode='auto')\n",
    "callbacks = [early_stopping, reduce_LR, model_checkpoint]\n",
    "        \n",
    "model = build_lstm_time_model(hidden_1, hidden_2, counting_hidden_1,\\\n",
    "                              counting_dense_1, counting_dense_2,\\\n",
    "                              learning_rate=lr, optimizer=optimizer, dropout=dropout)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "H_train = model.fit_generator(train_gen, validation_data=val_gen, validation_steps=1,\\\n",
    "                              steps_per_epoch=n_bins, epochs=epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test:  [[0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]\n",
      " [0.7172224 0.7024911]]\n",
      "Predicted Count:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_gen = test_generator(x_test, y_test)\n",
    "predicted_test = model.predict_generator(test_gen, steps=len(seqs_test))\n",
    "print('Predicted Test: ', predicted_test)\n",
    "\n",
    "mu = predicted_test[:,0]\n",
    "theta = predicted_test[:,1]\n",
    "mode = np.floor(mu*((theta-1)/theta)).astype(np.int)\n",
    "print('Predicted Count: ', mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
