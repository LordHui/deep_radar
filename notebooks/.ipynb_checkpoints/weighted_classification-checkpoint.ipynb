{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import types\n",
    "import datetime\n",
    "import random\n",
    "import argparse\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_embedding_model(hidden_1, hidden_2, reshape=None):\n",
    "    \n",
    "    global input_shape\n",
    "    model = Sequential()\n",
    "    if reshape:\n",
    "        model.add(Reshape(reshape, input_shape=input_shape))\n",
    "        model.add(LSTM(hidden_1, return_sequences=True))\n",
    "    else:\n",
    "        model.add(LSTM(hidden_1, return_sequences=True, input_shape=input_shape))\n",
    "    return model\n",
    "\n",
    "def lstm_counting_model(model, counting_hidden_1, counting_dense_1, counting_dense_2,\\\n",
    "                        kernel_initializer='normal', num_classes=6, dropout=None):\n",
    "        \n",
    "    model.add(Masking(mask_value=0.0, name='mask'))\n",
    "    model.add(LSTM(counting_hidden_1, return_sequences=False, name='counting_lstm_1'))\n",
    "    model.add(Dense(counting_dense_1, activation='relu', kernel_initializer=kernel_initializer, name='counting_dense_1'))\n",
    "    #model.add(Dense(counting_dense_2, activation='relu', kernel_initializer=kernel_initializer, name='counting_dense_2'))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='output'))\n",
    "    return model\n",
    "\n",
    "def build_lstm_time_model(hidden_1, hidden_2, counting_hidden_1, counting_dense_1, counting_dense_2,\\\n",
    "                          kernel_initializer='normal', num_classes=6, dropout=None):\n",
    "    \n",
    "    model = lstm_embedding_model(hidden_1, hidden_2, reshape=(-1, 2))\n",
    "    counting_model = lstm_counting_model(model, counting_hidden_1, counting_dense_1, counting_dense_2,\\\n",
    "                                         kernel_initializer=kernel_initializer, num_classes=num_classes,\\\n",
    "                                         dropout=dropout)\n",
    "    return counting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    weights = {}\n",
    "    for cls in classes:\n",
    "        x = np.where(y_train == cls)\n",
    "        weights[str(cls - 1)] = len(x[0])/len(y_train)\n",
    "    return weights\n",
    "\n",
    "def train_model(model, output_dir, train_gen, val_gen, n_bins, class_weights=None,\n",
    "                epochs=30, optimizer=None, learning_rate=1e-4):\n",
    "    \n",
    "    if optimizer == 'adam' or optimizer is None:\n",
    "        optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        \n",
    "    output_path = os.path.join(output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-4, verbose=5, mode='auto')\n",
    "    reduce_LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)        \n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(output_path, 'best_val_loss_model.h5'),\\\n",
    "                                       monitor='val_loss', verbose=5, save_best_only=True, mode='auto')\n",
    "    callbacks = [early_stopping, reduce_LR, model_checkpoint]\n",
    "\n",
    "#     model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\\\n",
    "#                    metrics=['sparse_categorical_accuracy']) #loss_weights=weights,\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    H_train = model.fit_generator(train_gen, validation_data=val_gen, validation_steps=1, class_weight=class_weights,\\\n",
    "                                  steps_per_epoch=n_bins, epochs=epochs, callbacks=callbacks)\n",
    "    return H_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'time'\n",
    "epochs = 5\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "optimizer = 'adam' #'adam', 'sgd'\n",
    "dropout = 0.2\n",
    "hidden_1 = 64\n",
    "hidden_2 = 32\n",
    "counting_hidden_1 = 32\n",
    "counting_dense_1 = 64\n",
    "counting_dense_2 = 16\n",
    "window = 256\n",
    "loss = 'crossent'\n",
    "model_type = 'lstm_time'\n",
    "base_path = '/scratch/sk7898/pedbike/window_256'\n",
    "out_dir = '/scratch/sk7898/radar_counting/models/'\n",
    "fileloc = os.path.join(base_path, 'downstream_time')\n",
    "    \n",
    "x_train, x_val, x_test, y_train, y_val, y_test, seqs_train, seqs_val, seqs_test = get_data(fileloc)\n",
    "n_bins = int(len(seqs_train)/batch_size)  \n",
    "\n",
    "y = list(y_train) + list(y_test) + list(y_val)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "#weights = get_weights(y_train) \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html: \n",
    "# n_samples / (n_classes * np.bincount(y))\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(list(y_train)), y_train)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(np.array(y).reshape(-1, 1))\n",
    "y_train = enc.transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_val = enc.transform(np.array(y_val).reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(np.array(y_test).reshape(-1, 1)).toarray()\n",
    "\n",
    "# print(class_weights) [  0.5505698    0.62540453   0.78556911   0.94730392   4.02604167 128.83333333]\n",
    "\n",
    "# print(weights)\n",
    "# {'0': 0.3067357512953368, '1': 0.26528497409326424, '2': 0.20518134715025907, \n",
    "# '3': 0.18031088082901556, '4': 0.039378238341968914, '5': 0.0031088082901554403}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/sk7898/miniconda3/envs/l3embedding-tf-14-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12/12 [==============================] - 62s 5s/step - loss: 1.7799 - accuracy: 0.3053 - val_loss: 1.7228 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.72284, saving model to /scratch/sk7898/radar_counting/models/crossent/lstm_time/20200226151159/best_val_loss_model.h5\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 61s 5s/step - loss: 1.7684 - accuracy: 0.3428 - val_loss: 1.7238 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.72284\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 61s 5s/step - loss: 1.7564 - accuracy: 0.3027 - val_loss: 1.6726 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.72284 to 1.67263, saving model to /scratch/sk7898/radar_counting/models/crossent/lstm_time/20200226151159/best_val_loss_model.h5\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 60s 5s/step - loss: 1.7429 - accuracy: 0.3027 - val_loss: 1.6425 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.67263 to 1.64252, saving model to /scratch/sk7898/radar_counting/models/crossent/lstm_time/20200226151159/best_val_loss_model.h5\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 61s 5s/step - loss: 1.7310 - accuracy: 0.3027 - val_loss: 1.6028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.64252 to 1.60284, saving model to /scratch/sk7898/radar_counting/models/crossent/lstm_time/20200226151159/best_val_loss_model.h5\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features = None, window*2\n",
    "input_shape=(n_timesteps, n_features)\n",
    "\n",
    "train_gen = train_generator(n_bins, x_train, y_train, seq_lengths=seqs_train, padding=True, padding_value=0.0)\n",
    "val_gen = val_generator(x_val, y_val)\n",
    "\n",
    "output_dir = os.path.join(out_dir + loss, model_type)\n",
    "model = build_lstm_time_model(hidden_1, hidden_2, counting_hidden_1, counting_dense_1,\\\n",
    "                              counting_dense_2, num_classes=num_classes, dropout=dropout)\n",
    "              \n",
    "history = train_model(model, output_dir, train_gen, val_gen, n_bins, class_weights=class_weights,\\\n",
    "                      epochs=epochs, optimizer=optimizer, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6665992736816406, 0.3229166567325592]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_gen = val_generator(x_test, y_test)\n",
    "test_score = model.evaluate_generator(test_gen, steps=len(seqs_test))\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
