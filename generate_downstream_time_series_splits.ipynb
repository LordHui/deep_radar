{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import PurePath\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data2IQ(filepath):\n",
    "    # Ref: https://github.com/dhruboroy29/MATLAB_Scripts/blob/6ded2d8d434239e3488ee79e02232dd7afff908c/Scripts/Data2IQ.m\n",
    "    # Read IQ streams from data\n",
    "    assert os.path.splitext(filepath)[1] == '.data' or os.path.splitext(filepath)[1] == '.bbs'\n",
    "    comp = np.fromfile(filepath, dtype=np.uint16)\n",
    "    I = comp[::2]\n",
    "    Q = comp[1::2]\n",
    "    try:\n",
    "        assert len(I) == len(Q)\n",
    "    except AssertionError as e:\n",
    "        e.args += (filepath,42)\n",
    "        raise\n",
    "\n",
    "    # Sanity check of I and Q samples (>4096, or abruptly different from prev. sample?)\n",
    "    for i in range(1,len(I)-1):\n",
    "        if I[i]>4096 or abs(int(I[i])-int(I[i-1]))>2000 and abs(int(I[i])-int(I[i+1]))>1500:\n",
    "            I[i] = I[i-1]\n",
    "        if Q[i]>4096 or abs(int(Q[i])-int(Q[i-1]))>2000 and abs(int(Q[i])-int(Q[i+1]))>1500:\n",
    "            Q[i] = Q[i-1]\n",
    "\n",
    "    return I,Q,len(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_series_data(fileloc, data_dir, win_len, fraction=None):\n",
    "    filenames = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    seqs = []\n",
    "    \n",
    "    [[filenames.append(os.path.join(os.path.join(fileloc, filestr), filename))\\\n",
    "      for filename in os.listdir(os.path.join(fileloc, filestr))] for filestr in data_dir]\n",
    "    \n",
    "    [labels.append(int((os.path.basename(fname).split('_')[-2]).split('p')[0])) for fname in filenames]\n",
    "    \n",
    "    for file in filenames:\n",
    "        I,Q,L = Data2IQ(file)\n",
    "        \n",
    "        windows = list(range(0, L - win_len + 1, win_len))\n",
    "        seqs.append(len(windows))\n",
    "        data_cut = np.zeros((len(windows), 2 * win_len), dtype=np.uint16)\n",
    "        \n",
    "        for k in range(len(windows)):\n",
    "            data_cut[k, ::2] = I[windows[k]: windows[k] + win_len]\n",
    "            data_cut[k, 1::2] = Q[windows[k]: windows[k] + win_len]\n",
    "        \n",
    "        data.append(data_cut)\n",
    "\n",
    "    if fraction:\n",
    "        indices = np.arange(len(filenames))\n",
    "        subset_indices = random.sample(indices.tolist(), k = int(fraction*len(filenames)))\n",
    "        filenames = [filenames[i] for i in subset_indices]\n",
    "        data = [data[i] for i in subset_indices]\n",
    "        labels = [labels[i] for i in subset_indices]\n",
    "        seqs = [seqs[i] for i in subset_indices]\n",
    "        \n",
    "    return filenames, data, labels, seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 256\n",
    "fileloc = '/scratch/sk7898/pedbike'\n",
    "classes = ['Human', 'Bike']\n",
    "\n",
    "humans_path = ['downstream/final_human_radial_full_cuts']\n",
    "bikes_path = ['downstream/final_bike_radial_full_cuts']\n",
    "data_dirs = [humans_path, bikes_path]\n",
    "\n",
    "data_labels = [0, 1]\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "filenames = []\n",
    "data = []\n",
    "labels = []\n",
    "seqs = []\n",
    "\n",
    "for label in data_labels:\n",
    "    f, d, l, s = get_time_series_data(fileloc, data_dirs[label], window)\n",
    "    filenames += f\n",
    "    data += d\n",
    "    labels += l\n",
    "    seqs += s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = fileloc\n",
    "path_prefix = 'window_256'\n",
    "\n",
    "indices = np.arange(len(filenames))\n",
    "\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val, seqs_train, seqs_val = train_test_split(data, labels,\\\n",
    "                                                                                                    indices, seqs,\\\n",
    "                                                                                                    test_size=val_split,\\\n",
    "                                                                                                    random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test, seqs_train, seqs_test = train_test_split(X_train, y_train,\\\n",
    "                                                                                                        indices_train,\\\n",
    "                                                                                                        seqs_train,\\\n",
    "                                                                                                        test_size=test_split,\\\n",
    "                                                                                                        random_state=42)\n",
    "\n",
    "files_train = [filenames[i] for i in indices_train]\n",
    "files_val = [filenames[i] for i in indices_val]\n",
    "files_test = [filenames[i] for i in indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train) == len(y_train) == len(seqs_train)\n",
    "assert len(X_test) == len(y_test) == len(seqs_test)\n",
    "assert len(X_val) == len(y_val) == len(seqs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the original data into train, val and test\n",
    "outdir = '/scratch/sk7898/pedbike/downstream'\n",
    "prefix = 'downstream'\n",
    "\n",
    "train_dir = os.path.join(outdir, path_prefix, prefix + \"_train\")\n",
    "val_dir = os.path.join(outdir, path_prefix, prefix + \"_val\")\n",
    "test_dir = os.path.join(outdir, path_prefix, prefix + \"_test\")\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(train_dir,cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir,cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir,cls), exist_ok=True)\n",
    "    \n",
    "for tr in files_train:\n",
    "    cur_class = PurePath(tr).name.split('_')[3]\n",
    "    copy(tr, os.path.join(train_dir, cur_class))\n",
    "\n",
    "for val in files_val:\n",
    "    cur_class = PurePath(val).name.split('_')[3]\n",
    "    copy(val, os.path.join(val_dir, cur_class))\n",
    "    \n",
    "for tst in files_test:\n",
    "    cur_class = PurePath(tst).name.split('_')[3]\n",
    "    copy(tst, os.path.join(test_dir, cur_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the time series data for train, val and test\n",
    "outdir = '/scratch/sk7898/pedbike/window_256/'\n",
    "path_prefix = 'downstream_time'\n",
    "\n",
    "os.makedirs(os.path.join(outdir, path_prefix), exist_ok=True)\n",
    "\n",
    "# Save train data\n",
    "np.save(os.path.join(outdir, path_prefix, \"train.npy\"), X_train)\n",
    "np.save(os.path.join(outdir, path_prefix, \"train_seqs.npy\"), seqs_train)\n",
    "np.save(os.path.join(outdir, path_prefix, \"train_lbls.npy\"), y_train)\n",
    "\n",
    "# Save validation data\n",
    "np.save(os.path.join(outdir, path_prefix, \"val.npy\"), X_val)\n",
    "np.save(os.path.join(outdir, path_prefix, \"val_seqs.npy\"), seqs_val)\n",
    "np.save(os.path.join(outdir, path_prefix, \"val_lbls.npy\"), y_val)\n",
    "\n",
    "# Save test data\n",
    "np.save(os.path.join(outdir, path_prefix, \"test.npy\"), X_val)\n",
    "np.save(os.path.join(outdir, path_prefix, \"test_seqs.npy\"), seqs_val)\n",
    "np.save(os.path.join(outdir, path_prefix, \"test_lbls.npy\"), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset Size: 955\n",
      "Train Dataset Size: 773\n",
      "Validation Dataset Size: 96\n",
      "Test Dataset Size: 86\n"
     ]
    }
   ],
   "source": [
    "print('Total Dataset Size:', len(filenames))\n",
    "print('Train Dataset Size:', len(files_train))\n",
    "print('Validation Dataset Size:', len(files_val))\n",
    "print('Test Dataset Size:', len(files_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**\n",
    "\n",
    "*Humans: 374*\n",
    "*Bikes: 398*\n",
    "    \n",
    "**Validation**\n",
    "\n",
    "*Humans: 45*\n",
    "*Bikes: 51*\n",
    "    \n",
    "**Test**\n",
    "\n",
    "*Humans: 45*\n",
    "*Bikes: 41*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the files\n",
    "# fileloc = '/scratch/sk7898/pedbike/window_256/upstream'\n",
    "# filestrs = ['Human/human_cuts_stft', 'Nonhuman/non_human_cuts_stft']\n",
    "# classes = ['human', 'non_human']\n",
    "# new_classes = ['Human', 'Nonhuman']\n",
    "\n",
    "# for filestr, cur_class, new_class in zip(filestrs, classes, new_classes):\n",
    "#     for f in os.listdir(os.path.join(fileloc, filestr)):\n",
    "#         old_file = os.path.join(fileloc, filestr, f)\n",
    "#         new_file = os.path.join(fileloc, filestr, os.path.basename(f).replace(cur_class, new_class))\n",
    "#         os.rename(old_file, new_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
